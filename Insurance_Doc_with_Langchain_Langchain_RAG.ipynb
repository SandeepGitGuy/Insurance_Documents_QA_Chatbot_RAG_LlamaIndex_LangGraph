{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%pip install langchain\n",
    "#%pip install langchain_openai\n",
    "\n",
    "# Import Necessary Libraries\n",
    "\n",
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "import nest_asyncio\n",
    "from diskcache import Cache\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nested asyncio to allow for nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables from the specified .env file\n",
    "load_dotenv(r\"C:\\Users\\sandy\\Downloads\\Insurance_Doc_RAG_With_LangchainLlamaIndex\\keys.env\")\n",
    "\n",
    "# Retrieve the OpenAI API key from the environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize a persistent Chroma database client with the specified path\n",
    "chroma_client = chromadb.PersistentClient(path=r\"C:\\Users\\sandy\\Downloads\\Insurance_Doc_RAG_With_LangchainLlamaIndex\\chroma.db\")\n",
    "\n",
    "# Define the path to the directory containing PDF documents to be processed\n",
    "pdf_dir_path = r\"C:\\Users\\sandy\\Downloads\\Insurance_Doc_RAG_With_LangchainLlamaIndex\\New folder\"\n",
    "\n",
    "# Set the language model to be used for processing\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize a cache to store results, with a specified cache directory\n",
    "cache = Cache(\"./cache_V2\")\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma_client.delete_collection(\"Insurance_Doc_RAG_LlamaIndex_Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(pdf_dir_path, storage_context):\n",
    "    \"\"\"Builds a vector store index from PDF documents.\n",
    "\n",
    "    Args:\n",
    "        pdf_dir_path (str): The path to the directory containing PDF documents to be indexed.\n",
    "        storage_context (StorageContext): The context for storing the vector index.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The created vector store index containing the processed documents.\n",
    "    \"\"\"  \n",
    "    # Load documents from the specified directory\n",
    "    docs = SimpleDirectoryReader(pdf_dir_path).load_data()\n",
    "    \n",
    "    # Create an ingestion pipeline with specified transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            # Split documents into sentences with specified chunk size and overlap\n",
    "            SentenceSplitter(chunk_size=2048, chunk_overlap=0),\n",
    "            # Extract titles from the documents\n",
    "            TitleExtractor(),\n",
    "            # Use OpenAI's embedding model for text embedding\n",
    "            OpenAIEmbedding(model_name=\"text-embedding-ada-002\"),\n",
    "            #HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline on the loaded documents to create nodes\n",
    "    nodes = pipeline.run(documents=docs)\n",
    "    \n",
    "    # Create a vector store index from the nodes and the provided storage context\n",
    "    index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
    "\n",
    "    return index  # Return the created index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_retrieval(query, index):\n",
    "    \"\"\"Retrieves data based on the provided query from the specified index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string used to search for relevant data.\n",
    "        index (VectorStoreIndex): The index from which to retrieve data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of results matching the query.\n",
    "    \"\"\"\n",
    "    # Convert the index into a retriever object for querying\n",
    "    retriever = index.as_retriever()\n",
    "    \n",
    "    # Retrieve results based on the provided query\n",
    "    results = retriever.retrieve(query)\n",
    "    \n",
    "    return results  # Return the retrieved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index():\n",
    "    \"\"\"Saves the vector store index to a Chroma collection.\n",
    "\n",
    "    This function creates a new collection in the Chroma database if it does not already exist,\n",
    "    initializes a vector store with that collection, and builds the index from the specified PDF documents.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The created vector store index containing the processed documents.\n",
    "    \"\"\"\n",
    "    # collection_list = chroma_client.list_collections()\n",
    "    # if \"Insurance_Doc_RAG_LlamaIndex_Langchain\" not in collection_list:\n",
    "\n",
    "    # Create a new collection in the Chroma database for storing the index\n",
    "    chroma_collection = chroma_client.create_collection(name=\"Insurance_Doc_RAG_LlamaIndex_Langchain\")\n",
    "    \n",
    "    # else:\n",
    "    #     chroma_collection = chroma_client.get_collection(name=\"Insurance_Doc_RAG_LlamaIndex_Langchain\")\n",
    "\n",
    "    # Initialize a Chroma vector store with the created collection\n",
    "    vector_store = ChromaVectorStore(\n",
    "        chroma_collection=chroma_collection,\n",
    "    )\n",
    "    \n",
    "    # Create a storage context using the vector store\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Build the index from the PDF documents in the specified directory\n",
    "    index = build_index(pdf_dir_path, storage_context)\n",
    "    \n",
    "    return index  # Return the created index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index():\n",
    "    \"\"\"Loads the vector store index from a Chroma collection.\n",
    "\n",
    "    This function checks if a specific collection exists in the Chroma database. \n",
    "    If it exists, it retrieves the collection and creates a vector store index from it.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex or None: The loaded vector store index if the collection exists, \n",
    "        otherwise None.\n",
    "    \"\"\"\n",
    "    # Retrieve the list of collections from the Chroma database\n",
    "    collection_list = chroma_client.list_collections()\n",
    "    \n",
    "    # Check if the specified collection exists in the list\n",
    "    if any(\"Insurance_Doc_RAG_LlamaIndex_Langchain\" in collection.name for collection in collection_list):\n",
    "        # Get the collection from the Chroma database\n",
    "        chroma_collection = chroma_client.get_collection(name=\"Insurance_Doc_RAG_LlamaIndex_Langchain\")\n",
    "        \n",
    "        # Initialize a Chroma vector store with the retrieved collection\n",
    "        vector_store = ChromaVectorStore(\n",
    "            chroma_collection=chroma_collection,\n",
    "        )\n",
    "        \n",
    "        # Create a storage context using the vector store\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        # Load the index from the vector store and storage context\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        #print(index)  # Debug Statement. Uncomment to print the index\n",
    "        return index  # Return the loaded index\n",
    "    else:\n",
    "        return None  # Return None if the collection does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to load the existing index from the Chroma collection\n",
    "index = load_index()\n",
    "\n",
    "# Uncomment the following line to print the loaded index for debugging\n",
    "#print(index)\n",
    "\n",
    "# Check if the index was not found (i.e., it is None)\n",
    "if index is None:\n",
    "    # If the index does not exist, create and save a new index\n",
    "    index = save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_docs(query):\n",
    "    \"\"\"Retrieves documents based on the provided query, utilizing a cache for efficiency.\n",
    "\n",
    "    This function first checks if the results for the given query are already cached. \n",
    "    If cached results are found, they are returned immediately. \n",
    "    If not, it retrieves the results from the index and caches them for future use.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string used to search for relevant documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of results matching the query.\n",
    "    \"\"\"\n",
    "    # Check if the results for the query are already in the cache\n",
    "    if cache.get(query) is not None:\n",
    "        return cache.get(query)  # Return cached results if available\n",
    "    \n",
    "    # Retrieve results from the index if not cached\n",
    "    results = data_retrieval(query, index)\n",
    "    \n",
    "    # Store the retrieved results in the cache with a specified expiration time\n",
    "    cache.set(query, results, expire=600)\n",
    "\n",
    "    return results  # Return the retrieved results\n",
    "\n",
    "# Uncomment the following line to test the function with a sample query\n",
    "# res = retrive_docs(\"what is Waiting Period and Exclusions?\")\n",
    "# print(res)  # Uncomment to print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(res[1].node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# @tool\n",
    "# docs_tool = Tool(\n",
    "#     name=\"Cache and Document Search\",\n",
    "#     func=retrive_docs,\n",
    "#     description=\"Use this tool when you need to answer questions about policy documents.\",\n",
    "# )\n",
    "\n",
    "# search_tool = Tool(\n",
    "#     name=\"Internet_Search\",\n",
    "#     # func=lambda query: SerpAPIWrapper().run(query),\n",
    "#     func = SerpAPIWrapper(serpapi_api_key =\"663d549846fac3c10e2b7d0dfeed509b0923c171084ecf2b8b4574bef5b3683a\").run,\n",
    "#     description=\"Use this tool when you need to answer questions about normal things.\",\n",
    "# )\n",
    "\n",
    "# custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are an Question answering expert. The user will ask you a question/query. Answer question to the best of your ability.\n",
    "# Always use the Cache and Document Search tool to search for the answer first.\n",
    "# if you dont find a satisfactory answer, then use the Internet Search tool to search for the answer.\n",
    "# \"\"\"\n",
    "    \n",
    "# )\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# tools = [Doc_tool, search_tool]\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# agent = initialize_agent(tools = tools, \n",
    "#     llm = llm, \n",
    "#     prompt=custom_prompt, \n",
    "#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "#     verbose=True, \n",
    "#     memory=memory, \n",
    "#     agent_executor=AgentExecutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# doc_tool = create_retriever_tool(\n",
    "#     retrive_docs,\n",
    "#     \"Search_docs\",\n",
    "#     \"Search and return the relevant policy document data.\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_tool(query: str):\n",
    "    \"\"\"BACKUP TOOL - Only use this tool if docs_tool fails to provide relevant information \n",
    "    or returns incomplete results. This tool searches external sources.\"\"\"\n",
    "    # Use the SerpAPIWrapper to perform a search on external sources using the provided query\n",
    "    return SerpAPIWrapper(serpapi_api_key =\"663d549846fac3c10e2b7d0dfeed509b0923c171084ecf2b8b4574bef5b3683a\").run(query)\n",
    "\n",
    "@tool\n",
    "def docs_tool(query: str):\n",
    "    \"\"\"PREFERRED TOOL - Use this tool FIRST to search internal documentation and get information. \n",
    "    Only use other tools if this tool fails to provide relevant information.\"\"\"\n",
    "    # Retrieve documents based on the query using the internal retrieval function\n",
    "    return retrive_docs(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tools to be used, with docs_tool as the primary and search_tool as the backup\n",
    "tools = [docs_tool, search_tool]\n",
    "\n",
    "# Create a ToolNode that manages the defined tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Initialize the language model with the specified model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# Bind the tools to the language model for enhanced functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Annotated, List\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langgraph.graph import StateGraph, START, END\n",
    "# from langgraph.graph.message import add_messages\n",
    "# from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     messages: List[dict]\n",
    "\n",
    "# graph_builder = StateGraph(State)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     return {\"messages\" : [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# tool_node = ToolNode(\n",
    "#     tools =[docs_tool, search_tool]\n",
    "# )\n",
    "\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "# graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "# graph_builder.add_conditional_edges(\"chatbot\", tools_condition, \"tools\")\n",
    "\n",
    "# graph_builder.add_edge(START, \"chatbot\")\n",
    "# graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_with_tools.invoke(\"What is the procedure to claim the insurance?\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_node.invoke({\"messages\": [llm_with_tools.invoke(\"What is the procedure to claim the insurance?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in app.stream(\n",
    "#     {\"messages\": [(\"human\", \"what is Accelerated Critical Illness Benefit in the Insurance Doc?\")]}, stream_mode=\"values\"\n",
    "# ):\n",
    "#     chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for chunk in app.stream(\n",
    "#     {\"messages\": [(\"human\", \"what is Accelerated Critical Illness Benefit in the Insurance Doc?\")]}, stream_mode=\"values\"\n",
    "# ):\n",
    "#     data.append(chunk[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState):\n",
    "    \"\"\"Determines whether the conversation should continue based on the last message.\n",
    "\n",
    "    This function checks the last message in the conversation state to see if it contains any tool calls.\n",
    "    If tool calls are present, it indicates that the conversation should continue with tool usage.\n",
    "    Otherwise, it signals the end of the conversation.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): The current state of the conversation, including messages.\n",
    "\n",
    "    Returns:\n",
    "        str: \"tools\" if there are tool calls in the last message, otherwise returns END.\n",
    "    \"\"\"\n",
    "    # Extract the list of messages from the conversation state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get the last message in the conversation\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if the last message contains any tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"  # Indicate that the conversation should continue with tools\n",
    "    \n",
    "    return END  # Indicate that the conversation should end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Invokes the language model with the current conversation messages.\n",
    "\n",
    "    This function takes the current state of the conversation, extracts the messages,\n",
    "    and passes them to the language model for processing. It returns the model's response\n",
    "    in a structured format.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): The current state of the conversation, including messages.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the model's response wrapped in a list under the key \"messages\".\n",
    "    \"\"\"\n",
    "    # Extract the list of messages from the conversation state\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    #print(messages)\n",
    "    # Add a prompt to the messages.\n",
    "    prompt = \"\"\"\n",
    "    Give the reference of the document i.e page number and document name if you got the answer from the document. \n",
    "    If you got the answer from internet search, tell the same.\n",
    "    The response format should be:\n",
    "    <Answer>\n",
    "    <Reference>\n",
    "    reference format : Page Number | Page Title | Document Name\n",
    "    Example: Page 16 | Comprehensive Guide to Insurance Policy Administration: Managing Claims, Eligibility, and Member Information Requirements | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
    "    if you got the answer from internet search, Say \"Answer retrieved from internet search. Please contact the Agent/Customer Care executive for more details\" in Reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the language model with the current messages to get a response\n",
    "\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    messages.append(HumanMessage(content=prompt))\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return the response in a structured format\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a state graph to manage the flow of the conversation\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add a node for the agent that will handle the conversation logic\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "\n",
    "# Add a node for the tools that can be used during the conversation\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Define the starting point of the workflow, connecting the START node to the agent\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edges to determine the next step based on the agent's response\n",
    "# If the agent's response indicates tool usage, transition to the tools node; otherwise, end the conversation\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "\n",
    "# Connect the tools node back to the agent to allow for repeated tool usage\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the workflow into an executable application\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the workflow graph as a Mermaid diagram using the application's graph representation\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a thread ID to the configuration for saving the conversation.\n",
    "# This ensures that each conversation is saved and context is retained.\n",
    "\n",
    "config = {\"configurable\" : {\"thread_id\" : \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Insurance Documentation Chatbot. Please enter your query. Type 'exit' to quit.\n",
      "User:  is Alzheimer's Disease covered under critical illness benefits?\n",
      "Chatbot:  Yes, Alzheimer's Disease is covered under critical illness benefits as specified in the policy document.\n",
      "\n",
      "<Reference>\n",
      "Page 7 | Comprehensive Guide to Insurance Benefits: Coverage for Death, Accidental Death, and Critical Illness | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  what is Alzheimer's Disease?\n",
      "Chatbot:  Alzheimer's Disease is a progressive, neurodegenerative brain disorder that affects memory, thinking, and behavior. It is the most common cause of dementia, leading to a decline in cognitive functions severe enough to interfere with daily tasks.\n",
      "\n",
      "<Reference>\n",
      "Answer retrieved from internet search. Please contact the Agent/Customer Care executive for more details.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  what is Additional Sum Assured option?\n",
      "Chatbot:  The Additional Sum Assured option allows the Scheme Member to increase their coverage during the Coverage Term. This option is subject to certain conditions, including the requirement of an additional premium for the increased Sum Assured and the member meeting specific eligibility criteria.\n",
      "\n",
      "<Reference>\n",
      "Page 10 | Comprehensive Guide to Insurance Coverage Options, Benefits, and Termination Conditions for Scheme Members | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  exit\n",
      "Thanks for using Insurance Documentation Chatbot. Have a great day!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Welcome message for the user and instructions on how to exit the chatbot\n",
    "print(\"Welcome to Insurance Documentation Chatbot. Please enter your query. Type 'exit' to quit.\")\n",
    "\n",
    "# Get the user's input query and print it to the Output cell\n",
    "query = input()\n",
    "print(\"User: \", query)\n",
    "\n",
    "# Continue the conversation until the user types 'exit'\n",
    "while query != 'exit':\n",
    "    data = []  # Initialize a list to store the chatbot's responses\n",
    "    \n",
    "    # Stream the chatbot's responses based on the user's query\n",
    "    for chunk in app.stream({\"messages\": [(\"human\", query)]}, stream_mode=\"values\", config=config):\n",
    "        #print(chunk)  # Print the raw response chunk. Uncomment this for debugging\n",
    "        data.append(chunk[\"messages\"][-1].content)  # Append the chatbot's response to the data list\n",
    "    \n",
    "    # Print the last response from the chatbot\n",
    "    print(\"Chatbot: \", data[-1])\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Get the next query from the user\n",
    "    query = input()\n",
    "    print(\"User: \", query)\n",
    "\n",
    "# Thank the user for using the chatbot when they exit\n",
    "if 'exit' in query.lower():\n",
    "    print(\"Thanks for using Insurance Documentation Chatbot. Have a great day!\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stream_graph_updates(user_input: str):\n",
    "#     for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "#         for value in event.values():\n",
    "#             print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"User: \")\n",
    "#     if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "#         print(\"Goodbye!\")\n",
    "#         break\n",
    "\n",
    "#     stream_graph_updates(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Welcome to Insurance Documentation Chatbot. Please enter your query. Type 'exit' to quit.\")\n",
    "# chat_history = []\n",
    "# query = input()\n",
    "# print(\"User: \", query)\n",
    "# while query != 'exit':\n",
    "#     response = agent.invoke({\"input\": query})\n",
    "#     #chat_history.append({\"input\": query, \"output\": response['output']})\n",
    "#     #cache.set(response['input'], response['output'], expire=600)\n",
    "#     print(\"Chatbot: \", response['output'])\n",
    "#     query = input()\n",
    "#     print(\"User: \", query)\n",
    "\n",
    "# if 'exit' in query.lower():\n",
    "#     print(\"Thanks for using Insurance Documentation Chatbot. Have a great day!\")\n",
    "\n",
    "\n",
    "\n",
    "# #agent.run(\"What is the procedure to claim the insurance?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
